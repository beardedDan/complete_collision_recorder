{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"./fine_tuned_mistralai\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input text\n",
    "input_text = \"SUMMARIZE THIS TEXT: POLICE NARRATIVE WHILE UNIT1 WAS TURNING ONTO W44_, PEDESTRIAN UNIT2 WAS-CROSSING-THE-STREET- UTILIZING-THE -CROSSWALK WHILE ATTEMPTING TO EXECUTE THE TURN AT A LOW SPEED, UNIT? STRUCK PEDESTRIAN UNIT2 CAUSING HER TO FALL TO THE GROUND (FAILURE TO YIELD TO CROSS PEDESTRIAN) GCAT INDICATORS:  AT INTERSECTION TURN INVOLVED\"\n",
    "reference_text = \"A driver of a commercial truck turned left and struck a person crossing an intersection. Police officers did not ticket the driver.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(prompt, max_new_tokens=50, num_return_sequences=1):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,  # Limit only the output length\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,  # Enable sampling\n",
    "            top_k=50,        # Control randomness\n",
    "            top_p=0.95       # Control randomness\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sari(prompt, generated_texts, references):\n",
    "    \"\"\"\n",
    "    Evaluate the SARI score between the source (prompt), generated text, and references.\n",
    "    \n",
    "    Args:\n",
    "    - prompt: Original input sentence (source)\n",
    "    - generated_texts: List of generated sentences by the model\n",
    "    - references: List of simplified reference sentences\n",
    "    \n",
    "    Returns:\n",
    "    - SARI score\n",
    "    \"\"\"\n",
    "    sari_scores = []\n",
    "    for generated_text in generated_texts:\n",
    "        # Evaluate using the SARI score\n",
    "        sari_score = corpus_sari([prompt], [generated_text], [references])\n",
    "        sari_scores.append(sari_score)\n",
    "        print(f\"SARI score: {sari_score}\")\n",
    "    \n",
    "    return sari_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "generated_texts = generate_text(input_text, max_new_tokens=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARIZE THIS TEXT: POLICE NARRATIVE WHILE UNIT1 WAS TURNING ONTO W44_, PEDESTRIAN UNIT2 WAS-CROSSING-THE-STREET- UTILIZING-THE -CROSSWALK WHILE ATTEMPTING TO EXECUTE THE TURN AT A LOW SPEED, UNIT? STRUCK PEDESTRIAN UNIT2 CAUSING HER TO FALL TO THE GROUND (FAILURE TO YIELD TO CROSS PEDESTRIAN) GCAT INDICATORS:  AT INTERSECTION TURN INVOLVED\n",
      "SUMMARIZE THIS TEXT: POLICE NARRATIVE WHILE UNIT1 WAS TURNING ONTO W44_, PEDESTRIAN UNIT2 WAS-CROSSING-THE-STREET- UTILIZING-THE -CROSSWALK WHILE ATTEMPTING TO EXECUTE THE TURN AT A LOW SPEED, UNIT? STRUCK PEDESTRIAN UNIT2 CAUSING HER TO FALL TO THE GROUND (FAILURE TO YIELD TO CROSS PEDESTRIAN) GCAT INDICATORS:  AT INTERSECTION TURN INVOLVED an. police car in involved bicycle injuries involved pedest vehicle struck adult a driver injuries passenger crossing the of a crash in caused A intersection person crossing a a The a driver A the The person driver driver passenger passenger. crash the crash intersection an injuries the struck The injuries driver driver caused bicycle A a car a struck involved crash of of an. the of a driver car caused intersection The passenger the in driver an bicy the passenger The a an a. an a struck The intersection of car passenger a crash\n",
      "A driver of a commercial truck turned left and struck a person crossing an intersection. Police officers did not ticket the driver.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sari': 38.63365590352072}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "sari = load(\"sari\")\n",
    "sari_score = sari.compute(sources=[input_text], predictions=[generated_texts], references=[[reference_text]])\n",
    "print(input_text)\n",
    "print(generated_texts)\n",
    "print(reference_text)\n",
    "sari_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the HellaSwag dataset and a pre-trained model\n",
    "dataset = load_dataset(\"hellaswag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'generated', 'reference'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "data = {\n",
    "    'input': [input_text],\n",
    "    'generated': [generated_texts],\n",
    "    'reference': [reference_text]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a Hugging Face Dataset object\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "dataset_dict = DatasetDict({\"validation\": dataset})\n",
    "\n",
    "# Display the dataset\n",
    "print(dataset_dict['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on HellaSwag: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_hellaswag(dataset, model, tokenizer):\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate through the dataset\n",
    "    for example in dataset:\n",
    "        context = example['input']\n",
    "        choices = [example['generated']]  # Assuming generated text is a single choice (for multiple choices, adjust accordingly)\n",
    "        reference = example['reference']\n",
    "        \n",
    "        choice_scores = []\n",
    "        \n",
    "        # Evaluate each choice\n",
    "        for choice in choices:\n",
    "            input_text = context + \" \" + choice  # Concatenate context with choice\n",
    "            input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "            # Generate outputs (logits)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids)\n",
    "                logits = outputs.logits\n",
    "\n",
    "            # Compute the average score for the completion\n",
    "            choice_score = logits.mean().item()  # Or use logits.sum() depending on your scoring method\n",
    "            choice_scores.append(choice_score)\n",
    "\n",
    "        # Select the best choice (highest score)\n",
    "        predicted_choice_idx = torch.argmax(torch.tensor(choice_scores))\n",
    "\n",
    "        # Compare with the reference (assuming reference is the correct index or correct choice)\n",
    "        if predicted_choice_idx == reference:\n",
    "            correct_predictions += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct_predictions / total\n",
    "    print(f\"Accuracy on HellaSwag: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Now run the evaluation function\n",
    "evaluate_hellaswag(dataset, model, tokenizer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
