{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import General Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# General Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import logging\n",
    "# import warnings\n",
    "# from pathlib import Path\n",
    "\n",
    "# import torch\n",
    "# # import intel_extension_for_pytorch as ipex\n",
    "# from datasets import load_dataset\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     TrainingArguments,\n",
    "#     Trainer,\n",
    "#     DataCollatorForSeq2Seq,\n",
    "# )\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# # # Constants\n",
    "# BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "# DATA_PATH = \"b-mc2/sql-create-context\"\n",
    "# MODEL_PATH = \"./final_model\"\n",
    "# ADAPTER_PATH = \"./lora_adapters\"\n",
    "# DEVICE = torch.device(\"xpu\" if torch.xpu.is_available() else \"cpu\")\n",
    "# MODEL_CACHE_PATH = \"/home/common/data/Big_Data/GenAI/llm_models\"\n",
    "\n",
    "# # # Weights & Biases Configuration\n",
    "# ENABLE_WANDB = False\n",
    "\n",
    "# if ENABLE_WANDB:\n",
    "#     print(\"installing wandb...\")\n",
    "#     !{sys.executable} -m pip install -U --force \"wandb==0.15.12\" > /dev/null 2>&1\n",
    "#     print(\"installation complete...\")\n",
    "\n",
    "#     import wandb\n",
    "#     os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.abspath('')\n",
    "#     os.environ[\"WANDB_PROJECT\"] = f\"finetune-model-name_{BASE_MODEL.replace('/', '_')}\"\n",
    "#     os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "#     wandb.login()\n",
    "\n",
    "# # # Configuration\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"NUMEXPR_MAX_THREADS\"] = \"10\"\n",
    "# os.environ[\"ENABLE_SDP_FUSION\"] = \"true\"\n",
    "# os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Map Directories and Import NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory:  /media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder\n",
      "Src Directory:  /media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/src\n",
      "Data Directory:  /media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/data\n"
     ]
    }
   ],
   "source": [
    "# Map src directory\n",
    "import sys\n",
    "import os\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), \"../.\"))\n",
    "print(\"Root Directory: \", root_dir)\n",
    "src_dir = os.path.join(root_dir,\"src\")\n",
    "print(\"Src Directory: \", src_dir)\n",
    "sys.path.append(src_dir)\n",
    "data_dir = os.path.join(root_dir,\"data\")\n",
    "print(\"Data Directory: \", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # Check if a GPU is available\n",
    "# device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# # Initialize the summarizer with the device argument\n",
    "# summarizer = pipeline('text2text-generation', model='describeai/gemini', device=device)\n",
    "\n",
    "# code = \"print('hello world!')\"\n",
    "# response = summarizer(code, max_length=100, num_beams=3)\n",
    "\n",
    "# print(\"Summarized code: \" + response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_texts(row):\n",
    "    cad_text = row['CAD_TEXT'] if pd.notna(row['CAD_TEXT']) else \"\"\n",
    "    oh_text = row['OH1_TEXT'] if pd.notna(row['OH1_TEXT']) else \"\"\n",
    "    \n",
    "    if oh_text:  # If 'OH_TEXT' is not an empty string\n",
    "        return cad_text + \"POLICE NARRATIVE \\n\\n \" + oh_text\n",
    "    else:\n",
    "        return cad_text  # If 'OH_TEXT' is empty, return only 'CAD_TEXT'\n",
    "\n",
    "# Apply the function to the dataframe to create a new concatenated column\n",
    "training_df = pd.read_csv(os.path.join(data_dir, \"processed\", \"training_df.csv\"))\n",
    "training_df['concatenated_text'] = training_df.apply(concatenate_texts, axis=1)\n",
    "training_df = training_df[training_df[\"BIKE_CLE_TEXT\"].notnull() & training_df[\"concatenated_text\"].notnull()]\n",
    "training_df = training_df.dropna(subset=['concatenated_text', 'BIKE_CLE_TEXT'])\n",
    "training_df = training_df[training_df['concatenated_text'].str.strip() != '']\n",
    "training_df = training_df[training_df['BIKE_CLE_TEXT'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs['input_ids'][idx],\n",
    "            'attention_mask': self.inputs['attention_mask'][idx],\n",
    "            'labels': self.labels['input_ids'][idx]  # Use input_ids for labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")\n",
    "# # model.parallelize() \n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = torch.nn.DataParallel(model)\n",
    "\n",
    "# # Ensure that all inputs are on the same device as the model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Move the inputs and labels to the same device\n",
    "# def move_batch_to_device(batch, device):\n",
    "#     return {key: value.to(device) for key, value in batch.items()}\n",
    "\n",
    "# inputs = tokenizer(list(training_df['concatenated_text']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# labels = tokenizer(list(training_df['BIKE_CLE_TEXT']), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "# batch_size = 4\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10,\n",
    "#     save_steps=100,\n",
    "#     fp16=True  # Enable mixed precision if needed\n",
    "# )\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     batch = move_batch_to_device(batch, device)\n",
    "#     break  # Just to check one batch\n",
    "\n",
    "\n",
    "# # Now train the model with accelerate handling multi-GPU setup\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# config = PeftConfig.from_pretrained(\"spikecodes/ai-911-operator\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = PeftModel.from_pretrained(base_model, \"spikecodes/ai-911-operator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3455.361216730038"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# longest_observation = training_df['concatenated_text'].loc[training_df['concatenated_text'].str.len().idxmax()]\n",
    "\n",
    "average_length = training_df['concatenated_text'].str.len().mean()\n",
    "average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_model_and_tokenizer(base_model_id: str):\n",
    "#     local_model_id = base_model_id.replace(\"/\", \"--\")\n",
    "#     local_model_path = os.path.join(MODEL_CACHE_PATH, local_model_id)\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"Attempting to load model and tokenizer from: {local_model_path}\")\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             local_model_path,\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             low_cpu_mem_usage=True,\n",
    "#         )\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "#     except (OSError, PermissionError) as e:\n",
    "#         print(f\"Failed to load from {local_model_path}. Attempting to download...\")\n",
    "#         model = AutoModelForCausalLM.from_pretrained(\n",
    "#             base_model_id,\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             low_cpu_mem_usage=True,\n",
    "#         )\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "#     tokenizer.pad_token_id = 0\n",
    "#     tokenizer.padding_side = \"left\"\n",
    "#     return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# def generate_prompt_911(messages):\n",
    "#     prompt = \"You are reviewing descriptions of collisions between vehicles and pedestrians or bicycles. Your job is to summarize narratives from dispatch reports and police narratives.\\n\\n\"\n",
    "#     for message in messages:\n",
    "#         role = \"Reviewer\" if message['role'] == 'assistant' else \"CAD\"\n",
    "#         prompt += f\"{role}: {message['content']}\\n\"\n",
    "#     prompt += \"Reviewer:\"\n",
    "#     return prompt\n",
    "\n",
    "# class FineTuner:\n",
    "#     def __init__(self, base_model_id: str, model_path: str, device: torch.device):\n",
    "#         self.base_model_id = base_model_id\n",
    "#         self.model_path = model_path\n",
    "#         self.device = device\n",
    "#         self.model, self.tokenizer = setup_model_and_tokenizer(base_model_id)\n",
    "\n",
    "#     # def tokenize_data(self, data_point, add_eos_token=True, cutoff_len=512):\n",
    "#     #     prompt = generate_prompt_911(data_point[\"messages\"])\n",
    "#     #     tokenized = self.tokenizer(\n",
    "#     #         prompt,\n",
    "#     #         truncation=True,\n",
    "#     #         max_length=cutoff_len,\n",
    "#     #         padding=False,\n",
    "#     #         return_tensors=None,\n",
    "#     #     )\n",
    "        \n",
    "#     #     if (\n",
    "#     #         tokenized[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n",
    "#     #         and add_eos_token\n",
    "#     #         and len(tokenized[\"input_ids\"]) < cutoff_len\n",
    "#     #     ):\n",
    "#     #         tokenized[\"input_ids\"].append(self.tokenizer.eos_token_id)\n",
    "#     #         tokenized[\"attention_mask\"].append(1)\n",
    "            \n",
    "#     #     tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "#     #     return tokenized\n",
    "    \n",
    "#     def tokenize_data(self, data_batch, add_eos_token=True, cutoff_len=512):\n",
    "#         # Generate prompts for the batch (list of texts)\n",
    "\n",
    "#         if isinstance(data_batch, dict):  # Single data point\n",
    "#             data_batch = [data_batch]\n",
    "\n",
    "\n",
    "#         prompts = [generate_prompt_911(data_point[\"messages\"]) for data_point in data_batch]\n",
    "        \n",
    "#         # Batch tokenization (tokenizing all prompts in parallel)\n",
    "#         tokenized_batch = self.tokenizer(\n",
    "#             prompts,\n",
    "#             truncation=True,\n",
    "#             max_length=cutoff_len,\n",
    "#             padding=\"max_length\",  # Use padding to the max length of the batch\n",
    "#             return_tensors=\"pt\",  # Return PyTorch tensors for batch processing\n",
    "#         )\n",
    "        \n",
    "#         # Check for EOS token and handle it in each sequence\n",
    "#         # if add_eos_token:\n",
    "#         #     for idx, input_ids in enumerate(tokenized_batch[\"input_ids\"]):\n",
    "#         #         if (\n",
    "#         #             input_ids[-1] != self.tokenizer.eos_token_id\n",
    "#         #             and len(input_ids) < cutoff_len\n",
    "#         #         ):\n",
    "#         #             tokenized_batch[\"input_ids\"][idx][-1] = self.tokenizer.eos_token_id\n",
    "#         #             tokenized_batch[\"attention_mask\"][idx][-1] = 1\n",
    "        \n",
    "#         # Labels should be a copy of input_ids for causal language models\n",
    "#         tokenized_batch[\"labels\"] = tokenized_batch[\"input_ids\"].clone()\n",
    "\n",
    "#         return tokenized_batch\n",
    "\n",
    "\n",
    "\n",
    "#     # def prepare_data(self, data, val_set_size=100):\n",
    "#     #     # train_val_split = data[\"train\"].train_test_split(\n",
    "#     #     #     test_size=val_set_size, shuffle=True, seed=123\n",
    "#     #     # )\n",
    "#     #     # train_data = train_val_split[\"train\"].shuffle().map(self.tokenize_data)\n",
    "#     #     # val_data = train_val_split[\"test\"].shuffle().map(self.tokenize_data)\n",
    "\n",
    "#     #     train_val_split = data.train_test_split(\n",
    "#     #         test_size=val_set_size, shuffle=True, seed=123)\n",
    "\n",
    "#     #     train_data = train_val_split[\"train\"].shuffle(seed=123).map(self.tokenize_data)\n",
    "#     #     val_data = train_val_split[\"test\"].shuffle(seed=123).map(self.tokenize_data)\n",
    "\n",
    "#     #     return train_data, val_data\n",
    "\n",
    "\n",
    "#     def prepare_data(self, data, val_set_size=100):\n",
    "#         # Check if `data` is a valid Hugging Face dataset\n",
    "#         print(f\"Dataset before split: {data}\")\n",
    "\n",
    "#         # Filter out rows with missing 'text' or 'label' fields\n",
    "#         data = data.filter(lambda example: example['text'] is not None and example['label'] is not None)\n",
    "\n",
    "#         # Split dataset into training and validation sets\n",
    "#         train_val_split = data.train_test_split(test_size=val_set_size, shuffle=True, seed=123)\n",
    "\n",
    "#         # Debugging output to check dataset size\n",
    "#         print(f\"Training set size: {len(train_val_split['train'])}\")\n",
    "#         print(f\"Validation set size: {len(train_val_split['test'])}\")\n",
    "\n",
    "#         # Shuffle and tokenize the training and validation sets\n",
    "#         train_data = train_val_split[\"train\"].shuffle(seed=123).map(self.tokenize_data, batched=True, batch_size=16)\n",
    "#         val_data = train_val_split[\"test\"].shuffle(seed=123).map(self.tokenize_data, batched=True, batch_size=16)\n",
    "\n",
    "#         # Validate tokenized data\n",
    "#         print(f\"First tokenized example in train_data: {train_data[0]}\")\n",
    "#         print(f\"First tokenized example in val_data: {val_data[0]}\")\n",
    "\n",
    "#         return train_data, val_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def train_model(self, train_data, val_data, training_args):\n",
    "#         self.model = self.model.to(self.device)\n",
    "#         self.model.gradient_checkpointing_enable()\n",
    "        \n",
    "#         for name, module in self.model.named_modules():\n",
    "#             print(name)        \n",
    "\n",
    "#         lora_config = LoraConfig(\n",
    "#             r=16,\n",
    "#             lora_alpha=32,\n",
    "#             target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "#             lora_dropout=0.05,\n",
    "#             bias=\"none\",\n",
    "#             task_type=\"CAUSAL_LM\",\n",
    "#         )\n",
    "\n",
    "\n",
    "#         self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "#         trainer = Trainer(\n",
    "#             model=self.model,\n",
    "#             train_dataset=train_data,\n",
    "#             eval_dataset=val_data,\n",
    "#             args=training_args,\n",
    "#             # data_collator=DataCollatorForSeq2Seq(\n",
    "#             #     self.tokenizer,\n",
    "#             #     pad_to_multiple_of=8,\n",
    "#             #     return_tensors=\"pt\",\n",
    "#             #     padding=True,\n",
    "#             # ),\n",
    "#             data_collator = DataCollatorForLanguageModeling(\n",
    "#                 tokenizer=self.tokenizer,\n",
    "#                 mlm=False,  # Set to False since you're doing causal language modeling\n",
    "#                 pad_to_multiple_of=8\n",
    "#                 )\n",
    "#         )\n",
    "        \n",
    "#         self.model.config.use_cache = False\n",
    "#         trainer.train()\n",
    "#         self.model.save_pretrained(self.model_path)\n",
    "\n",
    "#     def finetune(self, data_path, training_args):\n",
    "#         print(\"LOADING DATASET\")\n",
    "\n",
    "#         data = Dataset.from_pandas(training_df[['concatenated_text', 'BIKE_CLE_TEXT']])\n",
    "#         data = data.rename_column(\"concatenated_text\", \"text\")\n",
    "#         data = data.rename_column(\"BIKE_CLE_TEXT\", \"label\")\n",
    "\n",
    "#         print(\"DONE LOADING DATASET\")\n",
    "#         print(\"PREPARING DATASET\")\n",
    "#         train_data, val_data = self.prepare_data(data)\n",
    "#         print(\"DONE PREPARING DATASET\")\n",
    "#         self.train_model(train_data, val_data, training_args)\n",
    "\n",
    "# def lets_finetune(\n",
    "#     device=DEVICE,\n",
    "#     model=BASE_MODEL,\n",
    "#     per_device_batch_size=1,\n",
    "#     gradient_accumulation_steps=24,\n",
    "#     warmup_steps=20,\n",
    "#     learning_rate=2e-5,\n",
    "#     max_steps=200,\n",
    "# ):\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"Training Parameters:\")\n",
    "#     print(f\"Foundation model: {BASE_MODEL}\")\n",
    "#     print(f\"Model save path: {MODEL_PATH}\")\n",
    "#     print(f\"Device used: {DEVICE}\")\n",
    "#     if DEVICE.type.startswith(\"xpu\"):\n",
    "#         print(f\"Intel GPU: {torch.xpu.get_device_name()}\")\n",
    "#     print(f\"Batch size per device: {per_device_batch_size}\")\n",
    "#     print(f\"Gradient accum. steps: {gradient_accumulation_steps}\")\n",
    "#     print(f\"Warmup steps: {warmup_steps}\")\n",
    "#     print(f\"Max steps: {max_steps}\")\n",
    "#     print(f\"Learning rate: {learning_rate}\")\n",
    "#     print(f\"{'='*60}\\n\")\n",
    "\n",
    "#     finetuner = FineTuner(base_model_id=model, model_path=MODEL_PATH, device=device)\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         per_device_train_batch_size=per_device_batch_size,\n",
    "#         gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#         warmup_steps=warmup_steps,\n",
    "#         max_steps=max_steps,\n",
    "#         learning_rate=learning_rate,\n",
    "#         bf16=True,\n",
    "#         use_ipex=True,\n",
    "#         logging_steps=20,\n",
    "#         save_strategy=\"steps\",\n",
    "#         save_steps=20,\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         eval_steps=20,\n",
    "#         optim=\"adamw_hf\",\n",
    "#         output_dir=ADAPTER_PATH,\n",
    "#         save_total_limit=3,\n",
    "#         load_best_model_at_end=True,\n",
    "#         ddp_find_unused_parameters=False,\n",
    "#         group_by_length=True,\n",
    "#         report_to=\"wandb\" if ENABLE_WANDB else [],\n",
    "#         fp16=False,\n",
    "#     )\n",
    "\n",
    "#     finetuner.finetune(DATA_PATH, training_args)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Parameters:\n",
      "Foundation model: mistralai/Mistral-7B-v0.1\n",
      "Model save path: ./final_model\n",
      "Device used: cpu\n",
      "Batch size per device: 1\n",
      "Gradient accum. steps: 24\n",
      "Warmup steps: 20\n",
      "Max steps: 200\n",
      "Learning rate: 2e-05\n",
      "============================================================\n",
      "\n",
      "Attempting to load model and tokenizer from: /home/common/data/Big_Data/GenAI/llm_models/mistralai--Mistral-7B-v0.1\n",
      "Failed to load from /home/common/data/Big_Data/GenAI/llm_models/mistralai--Mistral-7B-v0.1. Attempting to download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.74it/s]\n",
      "/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATASET\n",
      "DONE LOADING DATASET\n",
      "PREPARING DATASET\n",
      "Dataset before split: Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__'],\n",
      "    num_rows: 1315\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1315/1315 [00:00<00:00, 14206.74 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1215\n",
      "Validation set size: 100\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     lets_finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Tokenize the input and labels\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(\u001b[38;5;28mlist\u001b[39m(training_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcatenated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;28mlist\u001b[39m(training_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIKE_CLE_TEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Tokenize the input and labels\n",
    "inputs = tokenizer(list(training_df['concatenated_text']), padding=True, max_length=512, return_tensors=\"pt\")\n",
    "labels = tokenizer(list(training_df['BIKE_CLE_TEXT']), padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "labels = {key: value.to(device) for key, value in labels.items()}\n",
    "\n",
    "# Create a custom dataset\n",
    "dataset = CustomDataset(inputs, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Define training arguments and enable FP16 for mixed precision\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    fp16=True  # Enable mixed precision\n",
    ")\n",
    "\n",
    "# Initialize the Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Parameters:\n",
      "Foundation model: mistralai/Mistral-7B-v0.1\n",
      "Model save path: ./final_model\n",
      "Device used: cpu\n",
      "Batch size per device: 1\n",
      "Gradient accum. steps: 24\n",
      "Warmup steps: 20\n",
      "Max steps: 200\n",
      "Learning rate: 2e-05\n",
      "============================================================\n",
      "\n",
      "Attempting to load model and tokenizer from: /home/common/data/Big_Data/GenAI/llm_models/mistralai--Mistral-7B-v0.1\n",
      "Failed to load from /home/common/data/Big_Data/GenAI/llm_models/mistralai--Mistral-7B-v0.1. Attempting to download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.66it/s]\n",
      "/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⋄ LOADING DATASET\n",
      "⋄ DONE LOADING DATASET\n"
     ]
    }
   ],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/spikecodes/ai-911-operator/blob/main/dispatch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/training_args.py:1560: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/492 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (511) must match the size of tensor b (133) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 57\u001b[0m\n\u001b[1;32m     50\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     51\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     52\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     53\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3491\u001b[0m ):\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/trainer.py:3545\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3543\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m unwrapped_model\u001b[38;5;241m.\u001b[39m_get_name()\n\u001b[1;32m   3544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m-> 3545\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoother\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3547\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoother(outputs, labels)\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:578\u001b[0m, in \u001b[0;36mLabelSmoother.__call__\u001b[0;34m(self, model_output, labels, shift_labels)\u001b[0m\n\u001b[1;32m    575\u001b[0m smoothed_loss \u001b[38;5;241m=\u001b[39m log_probs\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    577\u001b[0m nll_loss\u001b[38;5;241m.\u001b[39mmasked_fill_(padding_mask, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m--> 578\u001b[0m \u001b[43msmoothed_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):\u001b[39;00m\n\u001b[1;32m    581\u001b[0m num_active_elements \u001b[38;5;241m=\u001b[39m padding_mask\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m-\u001b[39m padding_mask\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (511) must match the size of tensor b (133) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Tokenize the input and labels\n",
    "# Set the eos_token as the padding token\n",
    "\n",
    "inputs = tokenizer(list(training_df['concatenated_text']), \n",
    "                   padding=True, truncation=True, max_length=512, return_tensors=\"pt\",\n",
    "                   )\n",
    "labels = tokenizer(list(training_df['BIKE_CLE_TEXT']), \n",
    "                   padding=True, truncation=True, max_length=512, return_tensors=\"pt\",\n",
    "                   )\n",
    "\n",
    "# outputs = model(**inputs)\n",
    "# print(f\"Logits shape: {outputs.logits.shape}\")  # (batch_size, sequence_length, vocab_size)\n",
    "# print(f\"Labels shape: {labels['input_ids'].shape}\")  # (batch_size, sequence_length)\n",
    "\n",
    "# Shift the labels for causal language models\n",
    "# labels['input_ids'] = labels['input_ids'][:, 1:].contiguous()  # Shift labels by one token\n",
    "\n",
    "\n",
    "# Create a custom dataset\n",
    "dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Define training arguments, explicitly disabling GPU usage (no_cuda=True)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,  # Small batch size to fit in memory\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients over 8 steps\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    label_smoothing_factor=0.1,  # Apply label smoothing\n",
    "    no_cuda=True,  # Ensure training happens on CPU\n",
    "    fp16=False     # Disable mixed precision since you're using CPU\n",
    ")\n",
    "\n",
    "# Initialize the Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2855: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 5.78 GiB of which 111.88 MiB is free. Process 419769 has 2.96 GiB memory in use. Including non-PyTorch memory, this process has 1.76 GiB memory in use. Of the allocated memory 1.68 GiB is allocated by PyTorch, and 5.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 33\u001b[0m\n\u001b[1;32m     21\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     22\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Enable mixed precision\u001b[39;00m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Initialize the Hugging Face Trainer\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m     40\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/trainer.py:554\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    553\u001b[0m ):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/transformers/trainer.py:802\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 802\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/media/dzielinski06/HDD1/AI894 - Capstone/Complete Collision Recorder/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 5.78 GiB of which 111.88 MiB is free. Process 419769 has 2.96 GiB memory in use. Including non-PyTorch memory, this process has 1.76 GiB memory in use. Of the allocated memory 1.68 GiB is allocated by PyTorch, and 5.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Tokenize the input and labels\n",
    "inputs = tokenizer(list(training_df['concatenated_text']), padding=True, max_length=512, return_tensors=\"pt\")\n",
    "labels = tokenizer(list(training_df['BIKE_CLE_TEXT']), padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "labels = {key: value.to(device) for key, value in labels.items()}\n",
    "\n",
    "# Create a custom dataset\n",
    "dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Define training arguments and enable FP16 for mixed precision\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    fp16=True  # Enable mixed precision\n",
    ")\n",
    "\n",
    "# Initialize the Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenize the input and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gemini_llm/tokenizer_config.json',\n",
       " './fine_tuned_gemini_llm/special_tokens_map.json',\n",
       " './fine_tuned_gemini_llm/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_PEFT_llm\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_PEFT_llm\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
